{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77afd8b1-d66c-4b17-8561-e5950d26317b",
   "metadata": {},
   "source": [
    "#CSC2042S 2025\n",
    "## Assignment 2\n",
    "## Perceptron Image Classification\n",
    "### Maryam Abrahams (ABRMAR043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111b4480-88c1-4c80-af0b-0a8b6e5c6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089cbfc-1c2d-4216-b526-6d5f0fc7c837",
   "metadata": {},
   "source": [
    "## Task 1: Data Processing\n",
    "\n",
    "We start by loading the Simpsons-MNIST dataset from the directory structure and handling it so that we can create train/validation splits and prepare the data for the perceptron model.\n",
    "\n",
    "We'll do this by creating a function, load, which takes in the parameters (base_path: the path to the main dataset directory, mode: color vs gray, size: target image size ) and which returns a numpy array of converted images (both images and labels), as well as label_map: a dictionary mapping the folder names to numeric labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03950d65-8088-4a4a-ba8f-0b62a7b4e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset \n",
    "\n",
    "def load(base_path, mode=\"grayscale\", size = (28, 28)):\n",
    "\n",
    "    path = os.path.join(base_path, 'dataset', mode, 'train')\n",
    "    \n",
    "    characters = ['bart_simpson', 'charles_montgomery_burns', 'homer_simpson',\n",
    "                 'krusty_the_clown', 'lisa_simpson', 'marge_simpson',\n",
    "                 'milhouse_van_houten', 'moe_szyslak', 'ned_flanders',\n",
    "                 'principal_skinner']\n",
    "\n",
    "    label_map = {char: idx for idx, char in enumerate(characters)}\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    print(f\"Loading {mode} images from: {path}\\n\")\n",
    "\n",
    "    for character in characters: \n",
    "        char_path = os.path.join(path, character)\n",
    "\n",
    "        if not os.path.exists(char_path): \n",
    "            print(f\"Warning: Directory {char_path} does not exist\\n\")\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(char_path): \n",
    "            if file.endswith(\".jpg\"): \n",
    "                img_path = os.path.join(char_path, file)\n",
    "\n",
    "                try: \n",
    "                    with Image.open(img_path) as img: \n",
    "                        if mode == \"grayscale\": \n",
    "                            img = img.convert(\"L\") # make grescale\n",
    "                        else: \n",
    "                            img = img.convert(\"RGB\") # make colorful\n",
    "\n",
    "                        if img.size != size: \n",
    "                            img = img.resize(size)\n",
    "\n",
    "                        img_array = np.array(img)\n",
    "                        images.append(img_array) \n",
    "                        labels.append(label_map[character])\n",
    "                        \n",
    "                except Exception as e: \n",
    "                    print(f\"Error loading {img_path}: {e}\\n\")\n",
    "\n",
    "# Converting the images to numpy arrays\n",
    "    \n",
    "    X = np.array(images) \n",
    "    y = np.array(labels) \n",
    "\n",
    "    print(f\"Loaded {X.shape[0]} {mode} images with shape {X.shape[1:]}\\n\")\n",
    "    return X, y, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6a9ed8-214c-43b8-8dac-1029e4f62d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading grayscale images from: C:\\Users\\Yello\\OneDrive - University of Cape Town\\2025 Second Year\\Second Semester\\CSC2042S\\Assignment 2\\dataset\\grayscale\\train\n",
      "\n",
      "Loaded 8000 grayscale images with shape (28, 28)\n",
      "\n",
      "Loading rgb images from: C:\\Users\\Yello\\OneDrive - University of Cape Town\\2025 Second Year\\Second Semester\\CSC2042S\\Assignment 2\\dataset\\rgb\\train\n",
      "\n",
      "Loaded 8000 rgb images with shape (28, 28, 3)\n",
      "\n",
      "Grayscale data shape: (8000, 28, 28)\n",
      "RGB data shape: (8000, 28, 28, 3)\n",
      "Label mapping: {'bart_simpson': 0, 'charles_montgomery_burns': 1, 'homer_simpson': 2, 'krusty_the_clown': 3, 'lisa_simpson': 4, 'marge_simpson': 5, 'milhouse_van_houten': 6, 'moe_szyslak': 7, 'ned_flanders': 8, 'principal_skinner': 9}\n"
     ]
    }
   ],
   "source": [
    "# Loading both the gray and colored datasets: \n",
    "\n",
    "base_path = r\"C:\\Users\\Yello\\OneDrive - University of Cape Town\\2025 Second Year\\Second Semester\\CSC2042S\\Assignment 2\"\n",
    "\n",
    "try:\n",
    "    X_gray, y_gray, label_map = load(base_path, mode='grayscale')\n",
    "    X_rgb, y_rgb, _ = load(base_path, mode='rgb')\n",
    "    \n",
    "    print(f\"Grayscale data shape: {X_gray.shape}\")\n",
    "    print(f\"RGB data shape: {X_rgb.shape}\")\n",
    "    print(f\"Label mapping: {label_map}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please check that the dataset path is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95cb84c-d1df-4bb7-b589-d0719f222518",
   "metadata": {},
   "source": [
    "Next I'll create a function, splits, to create training and validation splits from the loaded data so that the stratified data is better understood. And in preparation for the perceptron implementation I'll normalize the data, creating multiple normalization options for better hyperparameter tuning later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a49323-9d4f-484a-bfbc-0b33f2503cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (6400, 784), Validation set: (1600, 784)\n",
      "Training set: (6400, 2352), Validation set: (1600, 2352)\n"
     ]
    }
   ],
   "source": [
    "# Training and validation splits\n",
    "\n",
    "def splits(X, y, test_size = 0.2, random_state = 42, flatten = True): \n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state = random_state, stratify = y)\n",
    "\n",
    "    if flatten: \n",
    "        if len(X_train.shape) > 2: \n",
    "            X_train = X_train.reshape(X_train.shape[0], -1) \n",
    "            X_val = X_val.reshape(X_val.shape[0], -1) \n",
    "\n",
    "    print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n",
    "    return X_train, X_val, y_train, y_val\n",
    "    \n",
    "X_gray_train, X_gray_val, y_gray_train, y_gray_val = splits(X_gray, y_gray)\n",
    "X_rgb_train, X_rgb_val, y_rgb_train, y_rgb_val = splits(X_rgb, y_rgb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e73fca8-f23a-445b-8086-b7919710d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized grayscale - Train range: [0.000, 1.00]\n"
     ]
    }
   ],
   "source": [
    "# Normalization options\n",
    "\n",
    "def normalize(X_train, X_val, method = \"none\"): \n",
    "\n",
    "    if method == \"minmax\": # to [0, 1]\n",
    "        X_train = X_train.astype('float32') / 255.0\n",
    "        X_val = X_val.astype('float32') / 255.0\n",
    "        \n",
    "    elif method == 'zscore':\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train.astype('float32'))\n",
    "        X_val = scaler.transform(X_val.astype('float32'))\n",
    "        \n",
    "    elif method == 'none':\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_val = X_val.astype('float32')\n",
    "    \n",
    "    return X_train, X_val\n",
    "\n",
    "X_gray_train_norm, X_gray_val_norm = normalize(X_gray_train, X_gray_val, 'minmax')\n",
    "print(f\"Normalized grayscale - Train range: [{X_gray_train_norm.min():.3f}, {X_gray_train_norm.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ff38c-7bfb-44cd-9944-1c67e334ad52",
   "metadata": {},
   "source": [
    "## Task 2: Multi-class Perceptron Implementation\n",
    "\n",
    "For our multi-class perceptron implementation, we  create both a binary perceptron and a multiclass perceptron class using the outline provided to us in the tutorials. For the multiclass implementation, it should be from scratch using a one-vs-rest approach, and for the binary version, we should implement the perceptron learning rule, plus a predict function to return a binary label, selecting the class with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa2f5cf-fb99-4565-8d3b-0c2b361b1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary perceptron class\n",
    "\n",
    "class BinaryPerceptron:\n",
    "\n",
    "    def __init__(self, n_features,learning_rate = 0.1, random_state = None):\n",
    "        self.weights = np.ones(n_features, dtype=float)\n",
    "        self.bias = 0.0\n",
    "        self.lr = learning_rate\n",
    "        self.errors = []\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.array(x, dtype=float)\n",
    "        net_input = np.dot(x, self.weights) + self.bias\n",
    "        return 1 if net_input >= 0 else 0\n",
    "\n",
    "    def apply_learning_rule(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "        error = y - y_hat\n",
    "        self.weights += self.lr * error * x\n",
    "        self.bias += self.lr * error\n",
    "        return abs(error)\n",
    "\n",
    "    def fit(self, X, y, max_epochs = 1000): \n",
    "        self.errors = []\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            total_error = 0\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                error = self.update(X[i], y[i])\n",
    "                total_error += error\n",
    "            \n",
    "            self.errors.append(total_error)\n",
    "            if total_error == 0:\n",
    "                print(f\"Converged after {epoch + 1} epochs\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"BinaryPerceptron(weights={self.weights}, bias={self.bias:.3f}, learning rate={self.lr})\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53bd4757-50d1-4d9a-bf16-6a65e9ce8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiclass perceptron\n",
    "\n",
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, n_features, n_classes =10, learning_rate =0.1, random_state = 42):\n",
    "        self.n_classes = n_classes\n",
    "        self.perceptrons = [\n",
    "            BinaryPerceptron(n_features, learning_rate, random_state + i) \n",
    "            for i in range(n_classes)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y, max_epochs=1000):\n",
    "        for i in range(self.n_classes):\n",
    "            print(f\"Training perceptron for class {i}...\")\n",
    "            y_binary = np.where(y == i, 1, 0)\n",
    "            self.perceptrons[i].fit(X, y_binary, max_epochs)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = np.zeros((len(X), self.n_classes))\n",
    "        for i, perceptron in enumerate(self.perceptrons):\n",
    "            scores[:, i] = np.dot(X, perceptron.weights) + perceptron.bias\n",
    "        return np.argmax(scores, axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab158e-9b03-449d-9afc-d301a1736d78",
   "metadata": {},
   "source": [
    "## Task 3: Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17064c51-7cdd-40de-8416-a88576aca03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c4958e2-ad48-463e-ac5c-35663fbc9443",
   "metadata": {},
   "source": [
    "## Task 4: Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cc768-b344-4593-9ede-907581e89ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d33a4cc-39cc-4f2b-b74b-19c33f5bfd00",
   "metadata": {},
   "source": [
    "## Task 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81137e62-bc52-48ff-af93-02c46fbecea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bfcc0fe-79ed-49d8-bd52-964a6abb0577",
   "metadata": {},
   "source": [
    "## Task 6: RGB vs grayscale analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b18872-a90c-4434-8b84-1d5b72bbc12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
