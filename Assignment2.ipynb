{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77afd8b1-d66c-4b17-8561-e5950d26317b",
   "metadata": {},
   "source": [
    "# CSC2042S 2025\n",
    "## Assignment 2 - Perceptron Image Classification\n",
    "### Maryam Abrahams (ABRMAR043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b4480-88c1-4c80-af0b-0a8b6e5c6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089cbfc-1c2d-4216-b526-6d5f0fc7c837",
   "metadata": {},
   "source": [
    "## Task 1: Data Processing\n",
    "\n",
    "We start by loading the Simpsons-MNIST dataset from the directory structure and handling it so that we can create train/validation splits and prepare the data for the perceptron model.\n",
    "\n",
    "We'll do this by creating a function, load, which takes in the parameters (base_path: the path to the main dataset directory, mode: color vs gray, size: target image size ) and which returns a numpy array of converted images (both images and labels), as well as label_map: a dictionary mapping the folder names to numeric labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03950d65-8088-4a4a-ba8f-0b62a7b4e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset \n",
    "\n",
    "def load(base_path, mode=\"grayscale\", size = (28, 28)):\n",
    "\n",
    "    path = os.path.join(base_path, 'dataset', mode, 'train')\n",
    "    \n",
    "    characters = ['bart_simpson', 'charles_montgomery_burns', 'homer_simpson',\n",
    "                 'krusty_the_clown', 'lisa_simpson', 'marge_simpson',\n",
    "                 'milhouse_van_houten', 'moe_szyslak', 'ned_flanders',\n",
    "                 'principal_skinner']\n",
    "\n",
    "    label_map = {char: idx for idx, char in enumerate(characters)}\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    print(f\"Loading {mode} images from: {path}\\n\")\n",
    "\n",
    "    for character in characters: \n",
    "        char_path = os.path.join(path, character)\n",
    "\n",
    "        if not os.path.exists(char_path): \n",
    "            print(f\"Warning: Directory {char_path} does not exist\\n\")\n",
    "            continue\n",
    "\n",
    "        for file in os.listdir(char_path): \n",
    "            if file.endswith(\".jpg\"): \n",
    "                img_path = os.path.join(char_path, file)\n",
    "\n",
    "                try: \n",
    "                    with Image.open(img_path) as img: \n",
    "                        if mode == \"grayscale\": \n",
    "                            img = img.convert(\"L\") # make grescale\n",
    "                        else: \n",
    "                            img = img.convert(\"RGB\") # make colorful\n",
    "\n",
    "                        if img.size != size: \n",
    "                            img = img.resize(size)\n",
    "\n",
    "                        img_array = np.array(img)\n",
    "                        images.append(img_array) \n",
    "                        labels.append(label_map[character])\n",
    "                        \n",
    "                except Exception as e: \n",
    "                    print(f\"Error loading {img_path}: {e}\\n\")\n",
    "\n",
    "# Converting the images to numpy arrays\n",
    "    \n",
    "    X = np.array(images) \n",
    "    y = np.array(labels) \n",
    "\n",
    "    print(f\"Loaded {X.shape[0]} {mode} images with shape {X.shape[1:]}\\n\")\n",
    "    return X, y, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6a9ed8-214c-43b8-8dac-1029e4f62d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading grayscale images from: C:\\Users\\Yello\\OneDrive - University of Cape Town\\2025 Second Year\\Second Semester\\CSC2042S\\Assignment 2\\dataset\\grayscale\\train\n",
      "\n",
      "Loaded 8000 grayscale images with shape (28, 28)\n",
      "\n",
      "Loading rgb images from: C:\\Users\\Yello\\OneDrive - University of Cape Town\\2025 Second Year\\Second Semester\\CSC2042S\\Assignment 2\\dataset\\rgb\\train\n",
      "\n",
      "Loaded 8000 rgb images with shape (28, 28, 3)\n",
      "\n",
      "Grayscale data shape: (8000, 28, 28)\n",
      "RGB data shape: (8000, 28, 28, 3)\n",
      "Label mapping: {'bart_simpson': 0, 'charles_montgomery_burns': 1, 'homer_simpson': 2, 'krusty_the_clown': 3, 'lisa_simpson': 4, 'marge_simpson': 5, 'milhouse_van_houten': 6, 'moe_szyslak': 7, 'ned_flanders': 8, 'principal_skinner': 9}\n"
     ]
    }
   ],
   "source": [
    "# Loading both the gray and colored datasets: \n",
    "\n",
    "base_path = r\"C:\\Users\\Yello\\OneDrive - University of Cape Town\\2025 Second Year\\Second Semester\\CSC2042S\\Assignment 2\"\n",
    "\n",
    "try:\n",
    "    X_gray, y_gray, label_map = load(base_path, mode='grayscale')\n",
    "    X_rgb, y_rgb, _ = load(base_path, mode='rgb')\n",
    "    \n",
    "    print(f\"Grayscale data shape: {X_gray.shape}\")\n",
    "    print(f\"RGB data shape: {X_rgb.shape}\")\n",
    "    print(f\"Label mapping: {label_map}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please check that the dataset path is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95cb84c-d1df-4bb7-b589-d0719f222518",
   "metadata": {},
   "source": [
    "Next I'll create a function, splits, to create training and validation splits from the loaded data so that the stratified data is better understood. And in preparation for the perceptron implementation I'll normalize the data, creating multiple normalization options for better hyperparameter tuning later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a49323-9d4f-484a-bfbc-0b33f2503cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (6400, 784), Validation set: (1600, 784)\n",
      "Training set: (6400, 2352), Validation set: (1600, 2352)\n"
     ]
    }
   ],
   "source": [
    "# Training and validation splits\n",
    "\n",
    "def splits(X, y, test_size = 0.2, random_state = 42, flatten = True): \n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state = random_state, stratify = y)\n",
    "\n",
    "    if flatten: \n",
    "        if len(X_train.shape) > 2: \n",
    "            X_train = X_train.reshape(X_train.shape[0], -1) \n",
    "            X_val = X_val.reshape(X_val.shape[0], -1) \n",
    "\n",
    "    print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}\")\n",
    "    return X_train, X_val, y_train, y_val\n",
    "    \n",
    "X_gray_train, X_gray_val, y_gray_train, y_gray_val = splits(X_gray, y_gray)\n",
    "X_rgb_train, X_rgb_val, y_rgb_train, y_rgb_val = splits(X_rgb, y_rgb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e73fca8-f23a-445b-8086-b7919710d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized grayscale - Train range: [0.000, 1.00]\n"
     ]
    }
   ],
   "source": [
    "# Normalization options\n",
    "\n",
    "def normalize(X_train, X_val, method = \"none\"): \n",
    "\n",
    "    if method == \"minmax\": # to [0, 1]\n",
    "        X_train = X_train.astype('float32') / 255.0\n",
    "        X_val = X_val.astype('float32') / 255.0\n",
    "        \n",
    "    elif method == 'zscore':\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train.astype('float32'))\n",
    "        X_val = scaler.transform(X_val.astype('float32'))\n",
    "        \n",
    "    elif method == 'none':\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_val = X_val.astype('float32')\n",
    "    \n",
    "    return X_train, X_val\n",
    "\n",
    "X_gray_train_norm, X_gray_val_norm = normalize(X_gray_train, X_gray_val, 'minmax')\n",
    "print(f\"Normalized grayscale - Train range: [{X_gray_train_norm.min():.3f}, {X_gray_train_norm.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ff38c-7bfb-44cd-9944-1c67e334ad52",
   "metadata": {},
   "source": [
    "## Task 2: Multi-class Perceptron Implementation\n",
    "\n",
    "For our multi-class perceptron implementation, we  create both a binary perceptron and a multiclass perceptron class using the outline provided to us in the tutorials. For the multiclass implementation, it should be from scratch using a one-vs-rest approach, and for the binary version, we should implement the perceptron learning rule, plus a predict function to return a binary label, selecting the class with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa2f5cf-fb99-4565-8d3b-0c2b361b1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary perceptron class\n",
    "\n",
    "class BinaryPerceptron:\n",
    "\n",
    "    def __init__(self, n_features,learning_rate = 0.1, random_state = None):\n",
    "        self.weights = np.ones(n_features, dtype=float)\n",
    "        self.bias = 0.0\n",
    "        self.lr = learning_rate\n",
    "        self.errors = []\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.array(x, dtype=float)\n",
    "        net_input = np.dot(x, self.weights) + self.bias\n",
    "        return 1 if net_input >= 0 else 0\n",
    "\n",
    "    def apply_learning_rule(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "        error = y - y_hat\n",
    "        self.weights += self.lr * error * x\n",
    "        self.bias += self.lr * error\n",
    "        return abs(error)\n",
    "\n",
    "    def fit(self, X, y, max_epochs = 1000): \n",
    "        self.errors = []\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            total_error = 0\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                error = self.apply_learning_rule(X[i], y[i])\n",
    "                total_error += error\n",
    "            \n",
    "            self.errors.append(total_error)\n",
    "            if total_error == 0:\n",
    "                print(f\"Converged after {epoch + 1} epochs\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"BinaryPerceptron(weights={self.weights}, bias={self.bias:.3f}, learning rate={self.lr})\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bd4757-50d1-4d9a-bf16-6a65e9ce8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass perceptron\n",
    "\n",
    "class MulticlassPerceptron:\n",
    "\n",
    "    def __init__(self, n_features, n_classes =10, learning_rate =0.1, random_state = 42):\n",
    "        self.n_classes = n_classes\n",
    "        self.perceptrons = [\n",
    "            BinaryPerceptron(n_features, learning_rate, random_state + i) \n",
    "            for i in range(n_classes)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y, max_epochs=1000):\n",
    "        for i in range(self.n_classes):\n",
    "            print(f\"Training perceptron for class {i}...\")\n",
    "            y_binary = np.where(y == i, 1, 0)\n",
    "            self.perceptrons[i].fit(X, y_binary, max_epochs)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = np.zeros((len(X), self.n_classes))\n",
    "        for i, perceptron in enumerate(self.perceptrons):\n",
    "            scores[:, i] = np.dot(X, perceptron.weights) + perceptron.bias\n",
    "        return np.argmax(scores, axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab158e-9b03-449d-9afc-d301a1736d78",
   "metadata": {},
   "source": [
    "## Task 3: Training \n",
    "\n",
    "We seek to implement a training loop to find optimal weights and learning rules, the hyperparameters. By investigating two stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816e4e44-ab37-4424-92a5-4c023ee9835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "class EnhancedBinaryPerceptron(BinaryPerceptron): \n",
    "\n",
    "    def __init__(self, n_features, learning_rate=0.1, random_state=None): \n",
    "        super().__init__(n_features, learning_rate, random_state)\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val =None, max_epochs = 1000, error_threshold=0.0, patience=5, verbose=True): \n",
    "        self.errors = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "        # Collecting for early stopping: \n",
    "        best_weights = self.weights.copy()\n",
    "        best_bias = self.bias\n",
    "        best_val_acc = 0 \n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "\n",
    "            indices = np.random.permutation(len(X))\n",
    "            X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "            \n",
    "            total_error = 0\n",
    "            for i in range(len(X)):\n",
    "                error = self.apply_learning_rule(X_shuffled[i], y_shuffled[i])\n",
    "                total_error += error\n",
    "            self.errors.append(total_error)\n",
    "\n",
    "            if X_val is not None and y_val is not None: \n",
    "                val_acc = self.accuracy(X_val, y_val)\n",
    "                self.val_accuracies.append(val_acc)\n",
    "                \n",
    "                # Early stop logic: \n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_weights = self.weights.copy()\n",
    "                    best_bias = self.bias\n",
    "                    patience_counter = 0\n",
    "                else: \n",
    "                    patience_counter += 1\n",
    "                if patience_counter >= patience: \n",
    "                    if verbose: \n",
    "                        print(f\"Early stopping at epoch {epoch+1}, best accuracy value: {best_val_acc:.4f}\")\n",
    "                        self.weights = best_weights\n",
    "                        self.bias = best_bias\n",
    "                        break\n",
    "                    \n",
    "            if total_error <= error_threshold:\n",
    "                if verbose: \n",
    "                    print(f\"Converged after {epoch + 1} epochs\")\n",
    "                break\n",
    "            if verbose and (epoch + 1) % 100 == 0: \n",
    "                print(f\"Epoch {epoch+1}, Training error: {total_error}\")\n",
    "                \n",
    "        if verbose and epoch == max_epochs - 1: \n",
    "            print(f\"Reached maximum epochs ({max_epochs})\")\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def accuracy(self, X, y): \n",
    "        predictions = np.array([self.predict(x) for x in X])\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "    def predict_score(self, x): \n",
    "        return np.dot(x, self.weights) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7e1036-40a4-4a23-bd34-8d8ef94ddb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the stop criteria strategies for the Bart Simpson class: \n",
      "\n",
      "=== Testing: Fixed Epochs (1000) ===\n",
      "Epoch 100, Training error: 926\n",
      "Epoch 200, Training error: 911\n",
      "Epoch 300, Training error: 911\n",
      "Epoch 400, Training error: 918\n",
      "Epoch 500, Training error: 924\n",
      "Reached maximum epochs (500)\n",
      "\n",
      "=== Testing: Error Threshold (0 errors) ===\n",
      "Epoch 100, Training error: 936\n",
      "Epoch 200, Training error: 910\n",
      "Epoch 300, Training error: 911\n",
      "Epoch 400, Training error: 897\n",
      "Epoch 500, Training error: 891\n",
      "Epoch 600, Training error: 914\n",
      "Epoch 700, Training error: 936\n",
      "Epoch 800, Training error: 892\n",
      "Epoch 900, Training error: 896\n",
      "Epoch 1000, Training error: 897\n",
      "Reached maximum epochs (1000)\n",
      "\n",
      "=== Testing: Early Stopping (patience=10) ===\n",
      "Early stopping at epoch 22, best accuracy value: 0.9000\n",
      "\n",
      "=== Comparison of Stopping Criteria ===\n",
      "                       strategy  epochs  final_error  final_accuracy\n",
      "0           Fixed Epochs (1000)     500          924        0.881875\n",
      "1    Error Threshold (0 errors)    1000          897        0.895625\n",
      "2  Early Stopping (patience=10)      22          966        0.900000\n"
     ]
    }
   ],
   "source": [
    "# Investigating stop conditions\n",
    "\n",
    "class_names = {\n",
    "    0: 'Bart Simpson',\n",
    "    1: 'Charles Montgomery Burns', \n",
    "    2: 'Homer Simpson',\n",
    "    3: 'Krusty the Clown',\n",
    "    4: 'Lisa Simpson',\n",
    "    5: 'Marge Simpson',\n",
    "    6: 'Milhouse Van Houten',\n",
    "    7: 'Moe Szyslak',\n",
    "    8: 'Ned Flanders',\n",
    "    9: 'Principal Skinner' \n",
    "}\n",
    "\n",
    "# Testing the stopping criteria for one class only, 0: Bart Simpson\n",
    "\n",
    "class_index = 0\n",
    "y_binary_train = np.where(y_gray_train == class_index, 1, 0)\n",
    "y_binary_val = np.where(y_gray_val == class_index, 1, 0)\n",
    "\n",
    "strategies = [\n",
    "    {'name': 'Fixed Epochs (1000)', 'max_epochs': 500, 'error_threshold': -1, 'patience': 1000},\n",
    "    {'name': 'Error Threshold (0 errors)', 'max_epochs': 1000, 'error_threshold': 0, 'patience': 1000},\n",
    "    {'name': 'Early Stopping (patience=10)', 'max_epochs': 500, 'error_threshold': -1, 'patience': 10}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Testing the stop criteria strategies for the Bart Simpson class: \")\n",
    "\n",
    "for strategy in strategies: \n",
    "    print(f\"\\n=== Testing: {strategy['name']} ===\")\n",
    "    \n",
    "    perceptron = EnhancedBinaryPerceptron( n_features=X_gray_train_norm.shape[1], learning_rate=0.1, random_state=42 )\n",
    "    \n",
    "    perceptron.fit( X_gray_train_norm, y_binary_train, X_val=X_gray_val_norm, y_val=y_binary_val, max_epochs=strategy['max_epochs'], error_threshold=strategy['error_threshold'], patience=strategy['patience'], verbose=True )\n",
    "\n",
    "    results.append({\n",
    "    'strategy': strategy['name'],\n",
    "    'epochs': len(perceptron.errors),\n",
    "    'final_error': perceptron.errors[-1],\n",
    "    'final_accuracy': perceptron.accuracy(X_gray_val_norm, y_binary_val)\n",
    "    })\n",
    "\n",
    "# Comparing and Analyzing results\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Comparison of Stopping Criteria ===\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4958e2-ad48-463e-ac5c-35663fbc9443",
   "metadata": {},
   "source": [
    "## Task 4: Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "629cc768-b344-4593-9ede-907581e89ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunable perceptron modelling \n",
    "\n",
    "class TunableBinaryPerceptron: \n",
    "    \n",
    "    def __init__(self, n_features, learning_rate=0.1, init_strategy='ones', random_state=None):\n",
    "        # Initialize weights based on strategy\n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "        \n",
    "        if init_strategy == 'zero':\n",
    "            self.weights = np.zeros(n_features, dtype=float)\n",
    "        elif init_strategy == 'constant':\n",
    "            self.weights = np.ones(n_features, dtype=float) * 0.01\n",
    "        elif init_strategy == 'uniform':\n",
    "            self.weights = np.random.uniform(-0.01, 0.01, n_features)\n",
    "        elif init_strategy == 'gaussian':\n",
    "            self.weights = np.random.normal(0, 0.01, n_features)\n",
    "        else:  # 'ones' - same as your original\n",
    "            self.weights = np.ones(n_features, dtype=float)\n",
    "        \n",
    "        self.bias = 0.0\n",
    "        self.lr = learning_rate\n",
    "        self.errors = []\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.array(x, dtype=float)\n",
    "        net_input = np.dot(x, self.weights) + self.bias\n",
    "        return 1 if net_input >= 0 else 0\n",
    "    \n",
    "    def apply_learning_rule(self, x, y):\n",
    "        y_hat = self.predict(x)\n",
    "        error = y - y_hat\n",
    "        self.weights += self.lr * error * x\n",
    "        self.bias += self.lr * error\n",
    "        return abs(error)\n",
    "    \n",
    "    def fit(self, X, y, max_epochs=1000): \n",
    "        self.errors = []\n",
    "        for epoch in range(max_epochs):\n",
    "            total_error = 0\n",
    "            \n",
    "            for i in range(len(X)):\n",
    "                error = self.apply_learning_rule(X[i], y[i])\n",
    "                total_error += error\n",
    "            \n",
    "            self.errors.append(total_error)\n",
    "            if total_error == 0:\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "\n",
    "#_________________________________________________________________________________________________________\n",
    "\n",
    "class TunableMulticlassPerceptron: \n",
    "    \n",
    "    def __init__(self, n_features, n_classes=10, learning_rate=0.1, init_strategy='ones', random_state=42):\n",
    "        self.n_classes = n_classes\n",
    "        self.perceptrons = [\n",
    "            TunableBinaryPerceptron(n_features, learning_rate, init_strategy, random_state + i) \n",
    "            for i in range(n_classes)\n",
    "        ]\n",
    "    \n",
    "    def fit(self, X, y, max_epochs=1000, verbose=False):\n",
    "        for i in range(self.n_classes):\n",
    "            if verbose:\n",
    "                print(f\"Training perceptron for class {i}...\")\n",
    "            y_binary = np.where(y == i, 1, 0)\n",
    "            self.perceptrons[i].fit(X, y_binary, max_epochs)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = np.zeros((len(X), self.n_classes))\n",
    "        for i, perceptron in enumerate(self.perceptrons):\n",
    "            scores[:, i] = np.dot(X, perceptron.weights) + perceptron.bias\n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "601b0b51-d171-4911-8773-5d4e7d2f27da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TUNING GRAYSCALE MULTICLASS PERCEPTRON ===\n",
      "LR: 0.001, Init: zero, Norm: none -> Acc: 0.2119\n",
      "LR: 0.001, Init: zero, Norm: minmax -> Acc: 0.2050\n",
      "LR: 0.001, Init: zero, Norm: zscore -> Acc: 0.2013\n",
      "LR: 0.001, Init: constant, Norm: none -> Acc: 0.2044\n",
      "LR: 0.001, Init: constant, Norm: minmax -> Acc: 0.2137\n",
      "LR: 0.001, Init: constant, Norm: zscore -> Acc: 0.2000\n",
      "LR: 0.001, Init: uniform, Norm: none -> Acc: 0.1969\n",
      "LR: 0.001, Init: uniform, Norm: minmax -> Acc: 0.2075\n",
      "LR: 0.001, Init: uniform, Norm: zscore -> Acc: 0.2013\n",
      "LR: 0.001, Init: gaussian, Norm: none -> Acc: 0.2062\n",
      "LR: 0.001, Init: gaussian, Norm: minmax -> Acc: 0.2056\n",
      "LR: 0.001, Init: gaussian, Norm: zscore -> Acc: 0.1906\n",
      "LR: 0.01, Init: zero, Norm: none -> Acc: 0.2119\n",
      "LR: 0.01, Init: zero, Norm: minmax -> Acc: 0.2050\n",
      "LR: 0.01, Init: zero, Norm: zscore -> Acc: 0.2013\n",
      "LR: 0.01, Init: constant, Norm: none -> Acc: 0.2081\n",
      "LR: 0.01, Init: constant, Norm: minmax -> Acc: 0.1975\n",
      "LR: 0.01, Init: constant, Norm: zscore -> Acc: 0.1969\n",
      "LR: 0.01, Init: uniform, Norm: none -> Acc: 0.1988\n",
      "LR: 0.01, Init: uniform, Norm: minmax -> Acc: 0.2119\n",
      "LR: 0.01, Init: uniform, Norm: zscore -> Acc: 0.1988\n",
      "LR: 0.01, Init: gaussian, Norm: none -> Acc: 0.1975\n",
      "LR: 0.01, Init: gaussian, Norm: minmax -> Acc: 0.1925\n",
      "LR: 0.01, Init: gaussian, Norm: zscore -> Acc: 0.2056\n",
      "LR: 0.1, Init: zero, Norm: none -> Acc: 0.2119\n",
      "LR: 0.1, Init: zero, Norm: minmax -> Acc: 0.2050\n",
      "LR: 0.1, Init: zero, Norm: zscore -> Acc: 0.2013\n",
      "LR: 0.1, Init: constant, Norm: none -> Acc: 0.2006\n",
      "LR: 0.1, Init: constant, Norm: minmax -> Acc: 0.2094\n",
      "LR: 0.1, Init: constant, Norm: zscore -> Acc: 0.2050\n",
      "LR: 0.1, Init: uniform, Norm: none -> Acc: 0.2112\n",
      "LR: 0.1, Init: uniform, Norm: minmax -> Acc: 0.2062\n",
      "LR: 0.1, Init: uniform, Norm: zscore -> Acc: 0.2112\n",
      "LR: 0.1, Init: gaussian, Norm: none -> Acc: 0.2025\n",
      "LR: 0.1, Init: gaussian, Norm: minmax -> Acc: 0.1988\n",
      "LR: 0.1, Init: gaussian, Norm: zscore -> Acc: 0.2081\n",
      "\n",
      "Best Grayscale: LR=0.001, Init=constant, Norm=minmax, Acc=0.2137\n",
      "\n",
      "=== TUNING RGB MULTICLASS PERCEPTRON ===\n",
      "LR: 0.001, Init: zero, Norm: none -> Acc: 0.3312\n",
      "LR: 0.001, Init: zero, Norm: minmax -> Acc: 0.3344\n",
      "LR: 0.001, Init: zero, Norm: zscore -> Acc: 0.3575\n",
      "LR: 0.001, Init: constant, Norm: none -> Acc: 0.3456\n",
      "LR: 0.001, Init: constant, Norm: minmax -> Acc: 0.3325\n",
      "LR: 0.001, Init: constant, Norm: zscore -> Acc: 0.3569\n",
      "LR: 0.001, Init: uniform, Norm: none -> Acc: 0.3300\n",
      "LR: 0.001, Init: uniform, Norm: minmax -> Acc: 0.3406\n",
      "LR: 0.001, Init: uniform, Norm: zscore -> Acc: 0.3588\n",
      "LR: 0.001, Init: gaussian, Norm: none -> Acc: 0.3125\n",
      "LR: 0.001, Init: gaussian, Norm: minmax -> Acc: 0.3569\n",
      "LR: 0.001, Init: gaussian, Norm: zscore -> Acc: 0.3538\n",
      "LR: 0.01, Init: zero, Norm: none -> Acc: 0.3312\n",
      "LR: 0.01, Init: zero, Norm: minmax -> Acc: 0.3344\n",
      "LR: 0.01, Init: zero, Norm: zscore -> Acc: 0.3575\n",
      "LR: 0.01, Init: constant, Norm: none -> Acc: 0.3300\n",
      "LR: 0.01, Init: constant, Norm: minmax -> Acc: 0.3287\n",
      "LR: 0.01, Init: constant, Norm: zscore -> Acc: 0.3638\n",
      "LR: 0.01, Init: uniform, Norm: none -> Acc: 0.3362\n",
      "LR: 0.01, Init: uniform, Norm: minmax -> Acc: 0.3387\n",
      "LR: 0.01, Init: uniform, Norm: zscore -> Acc: 0.3656\n",
      "LR: 0.01, Init: gaussian, Norm: none -> Acc: 0.3412\n",
      "LR: 0.01, Init: gaussian, Norm: minmax -> Acc: 0.3969\n",
      "LR: 0.01, Init: gaussian, Norm: zscore -> Acc: 0.3581\n",
      "LR: 0.1, Init: zero, Norm: none -> Acc: 0.3312\n",
      "LR: 0.1, Init: zero, Norm: minmax -> Acc: 0.3344\n",
      "LR: 0.1, Init: zero, Norm: zscore -> Acc: 0.3575\n",
      "LR: 0.1, Init: constant, Norm: none -> Acc: 0.3425\n",
      "LR: 0.1, Init: constant, Norm: minmax -> Acc: 0.3869\n",
      "LR: 0.1, Init: constant, Norm: zscore -> Acc: 0.3563\n",
      "LR: 0.1, Init: uniform, Norm: none -> Acc: 0.3125\n",
      "LR: 0.1, Init: uniform, Norm: minmax -> Acc: 0.3581\n",
      "LR: 0.1, Init: uniform, Norm: zscore -> Acc: 0.3606\n",
      "LR: 0.1, Init: gaussian, Norm: none -> Acc: 0.3187\n",
      "LR: 0.1, Init: gaussian, Norm: minmax -> Acc: 0.3019\n",
      "LR: 0.1, Init: gaussian, Norm: zscore -> Acc: 0.3494\n",
      "\n",
      "Best RGB: LR=0.01, Init=gaussian, Norm=minmax, Acc=0.3969\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tune_multiclass_perceptron(X_train, y_train, X_val, y_val, modality_name):\n",
    "    tuning_results = []\n",
    "    \n",
    "    # Testing all combos\n",
    "    for lr in [0.001, 0.01, 0.1]:\n",
    "        for init_strat in ['zero', 'constant', 'uniform', 'gaussian']:\n",
    "            for norm_method in ['none', 'minmax', 'zscore']:\n",
    "                \n",
    "                X_train_norm, X_val_norm = normalize(X_train, X_val, norm_method)\n",
    "\n",
    "                model = TunableMulticlassPerceptron(\n",
    "                    n_features=X_train_norm.shape[1],\n",
    "                    n_classes=10,\n",
    "                    learning_rate=lr,\n",
    "                    init_strategy=init_strat,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                model.fit(X_train_norm, y_train, max_epochs=50, verbose=False)\n",
    "                val_accuracy = model.accuracy(X_val_norm, y_val)\n",
    "                \n",
    "                tuning_results.append({\n",
    "                    'learning_rate': lr,\n",
    "                    'init_strategy': init_strat,\n",
    "                    'normalization': norm_method,\n",
    "                    'accuracy': val_accuracy\n",
    "                })\n",
    "                \n",
    "                print(f\"LR: {lr}, Init: {init_strat}, Norm: {norm_method} -> Acc: {val_accuracy:.4f}\")\n",
    "    return pd.DataFrame(tuning_results)\n",
    "\n",
    "\n",
    "# Tuning grey multisclass perceptron: \n",
    "\n",
    "print(\"=== TUNING GRAYSCALE MULTICLASS PERCEPTRON ===\")\n",
    "gray_results = tune_multiclass_perceptron(\n",
    "    X_gray_train, y_gray_train, \n",
    "    X_gray_val, y_gray_val,\n",
    "    \"Grayscale\"\n",
    ")\n",
    "\n",
    "best_gray = gray_results.loc[gray_results['accuracy'].idxmax()]\n",
    "print(f\"\\nBest Grayscale: LR={best_gray['learning_rate']}, Init={best_gray['init_strategy']}, Norm={best_gray['normalization']}, Acc={best_gray['accuracy']:.4f}\")\n",
    "\n",
    "# Tuning RGB multiclass perceptron\n",
    "\n",
    "X_rgb_train_flat = X_rgb_train.reshape(X_rgb_train.shape[0], -1)\n",
    "X_rgb_val_flat = X_rgb_val.reshape(X_rgb_val.shape[0], -1)\n",
    "\n",
    "print(\"\\n=== TUNING RGB MULTICLASS PERCEPTRON ===\")\n",
    "rgb_results = tune_multiclass_perceptron(\n",
    "    X_rgb_train_flat, y_rgb_train,\n",
    "    X_rgb_val_flat, y_rgb_val, \n",
    "    \"RGB\"\n",
    ")\n",
    "\n",
    "best_rgb = rgb_results.loc[rgb_results['accuracy'].idxmax()]\n",
    "print(f\"\\nBest RGB: LR={best_rgb['learning_rate']}, Init={best_rgb['init_strategy']}, Norm={best_rgb['normalization']}, Acc={best_rgb['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68b9cd4a-0e0c-428c-b2db-2d1e674a17e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "HYPERPARAMETER TUNING RESULTS\n",
      "==================================================\n",
      "GRAYSCALE BEST: LR=0.001, Init=constant, Norm=minmax with Acc=0.2137\n",
      "RGB BEST:       LR=0.01, Init=gaussian, Norm=minmax with Acc=0.3969\n",
      "\n",
      "BEST LEARNING RATE:\n",
      "Gray: 0.001, RGB: 0.01\n",
      "\n",
      "BEST INITIALIZATION:\n",
      "Gray: constant, RGB: gaussian\n",
      "\n",
      "BEST NORMALIZATION:\n",
      "Gray: minmax, RGB: minmax\n",
      "\n",
      "ACCURACY COMPARISON:\n",
      "Grayscale: 0.2137, RGB: 0.3969\n",
      "Difference: 0.1831 (RGB better)\n"
     ]
    }
   ],
   "source": [
    "# Analysis\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"GRAYSCALE BEST: LR={best_gray['learning_rate']}, Init={best_gray['init_strategy']}, Norm={best_gray['normalization']} with Acc={best_gray['accuracy']:.4f}\")\n",
    "print(f\"RGB BEST:       LR={best_rgb['learning_rate']}, Init={best_rgb['init_strategy']}, Norm={best_rgb['normalization']} with Acc={best_rgb['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nBEST LEARNING RATE:\")\n",
    "print(f\"Gray: {best_gray['learning_rate']}, RGB: {best_rgb['learning_rate']}\")\n",
    "\n",
    "print(\"\\nBEST INITIALIZATION:\")\n",
    "print(f\"Gray: {best_gray['init_strategy']}, RGB: {best_rgb['init_strategy']}\")\n",
    "\n",
    "print(\"\\nBEST NORMALIZATION:\")\n",
    "print(f\"Gray: {best_gray['normalization']}, RGB: {best_rgb['normalization']}\")\n",
    "\n",
    "print(\"\\nACCURACY COMPARISON:\")\n",
    "print(f\"Grayscale: {best_gray['accuracy']:.4f}, RGB: {best_rgb['accuracy']:.4f}\")\n",
    "print(f\"Difference: {abs(best_gray['accuracy'] - best_rgb['accuracy']):.4f} ({'Grayscale' if best_gray['accuracy'] > best_rgb['accuracy'] else 'RGB'} better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33a4cc-39cc-4f2b-b74b-19c33f5bfd00",
   "metadata": {},
   "source": [
    "## Task 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81137e62-bc52-48ff-af93-02c46fbecea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test data only \n",
    "\n",
    "def load_test_data(base_path, mode=\"grayscale\", size=(28, 28)):\n",
    "    \n",
    "    original_load = load  \n",
    "\n",
    "    def load_test(base_path, mode, size):\n",
    "        path = os.path.join(base_path, 'dataset', mode, 'test')  # Changed to 'test'\n",
    "        characters = ['bart_simpson', 'charles_montgomery_burns', 'homer_simpson',\n",
    "                     'krusty_the_clown', 'lisa_simpson', 'marge_simpson',\n",
    "                     'milhouse_van_houten', 'moe_szyslak', 'ned_flanders',\n",
    "                     'principal_skinner']\n",
    "\n",
    "    X_test, y_test, _ = load_test(base_path, mode, size)\n",
    "    return X_test, y_test\n",
    "\n",
    "X_gray_test, y_gray_test = load_test_data(base_path, mode='grayscale')\n",
    "X_rgb_test, y_rgb_test = load_test_data(base_path, mode='rgb')\n",
    "\n",
    "X_gray_test_flat = X_gray_test.reshape(X_gray_test.shape[0], -1)\n",
    "X_rgb_test_flat = X_rgb_test.reshape(X_rgb_test.shape[0], -1)\n",
    "\n",
    "_, X_gray_test_norm = normalize(X_gray_test_flat, X_gray_test_flat, best_gray['normalization'])\n",
    "_, X_rgb_test_norm = normalize(X_rgb_test_flat, X_rgb_test_flat, best_rgb['normalization'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfcc0fe-79ed-49d8-bd52-964a6abb0577",
   "metadata": {},
   "source": [
    "## Task 6: RGB vs grayscale analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b18872-a90c-4434-8b84-1d5b72bbc12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
